?title kumofs
?author FURUHASHI Sadayuki

?css user.css

*?kumofs user guide

*kumofsとは
kumofsはkey-value型のデータを保存する非常に高速な分散ストレージです。1台か2台のManagerノードと、複数のServerノードでクラスタを構成します。
動作中にサーバーを追加することができ、読み・書き両方の性能をスケールアウトさせることができます。またデータのレプリケーションをサポートしており、一部のサーバーに障害が発生しても正常に動作し続けます。
memcached互換のプロトコルをサポートしており、テキスト・バイナリどちらのプロトコルも利用できます。

*インストール
//kumofsは以下のURLからダウンロードできます

kumofsをコンパイルして実行するには以下の環境が必要です：
-Linux >= 2.6.18
-Tokyo Cabinet >= 1.4.10
-MessagePack for C++ >= 0.3.1
-MessagePack for Ruby >= 0.3.1
-ruby >= 1.8.7
-libcrypto (openssl)
-zlib
-g++ >= 4.1 （コンパイルのみ）
-ragel >= 6.3 (コンパイルのみ)

 ./configure && make && make install でインストールできます。
>|sh|
$ ./configure
$ make
$ sudo make install
||<

**configureフラグ
:--with-tokyocabinet=DIR:Tokyo Cabinetがインストールされているディレクトリを指定する
:--with-msgpack=DIR:MessagePackがインストールされているディレクトリを指定する
:--with-tcmalloc[=DIR]:tcmallocとリンクする
:--enable-trace:画面を埋め尽くすほど冗長なデバッグ用のメッセージを出力するようにする

*チュートリアル
kumofsは、実際にデータを保存する''Server''ノード、Serverノード群を管理する''Manager''ノード、アプリケーションからのリクエストを中継する''Gateway''の３種類のプロセスから構成されます。それぞれ''kumo-server''、''kumo-manager''、''kumo-gateway''というコマンドが対応します。

kumo-gatewayはmemcachedプロトコルをサポートしています。kumofsを利用したいホストでkumo-gatewayを動作させておくことで、アプリケーションはlocalhostのkumo-gatewayにmemcachedクライアントライブラリを使って接続することでkumofsを利用できます。

たとえば '''s1''', '''s2''', '''s3''', '''s4''' の4台のホストをServerノード、'''m1''' をManagerノードとし、'''c1''', '''c2''' の2台から利用するには、以下のようにします：
>|sh|
[m1]$ kumo-manager -v -l m1
[s1]$ kumo-server  -v -m m1 -l s1 -s database.tch    # -mでManagerを指定する
[s2]$ kumo-server  -v -m m1 -l s2 -s database.tch    # -lは自ホストのアドレス
[s3]$ kumo-server  -v -m m1 -l s3 -s database.tch    # -sはデータベース名
[s4]$ kumo-server  -v -m m1 -l s4 -s database.tch    # -vは冗長なメッセージを出力
[c1]$ kumo-gateway -v -m m1 -t 11211    # 11211/tcpでmemcachedテキストプロトコル
[c2]$ kumo-gateway -v -m m1 -t 11211    # を待ち受ける
||<
kumo-managerとkumo-serverとkumo-gatewayを同じホスト上で動かすこともできます。たとえば '''m1''' と '''s1''' と '''c1''' は実際には同じホストでも大丈夫です。

kumo-managerは2台のホストで冗長化することができます。'''m1'''と'''m2'''の2台のホストでkumo-managerを起動するには以下のようにします：
>|sh|
[m1]$ kumo-manager -v -l m1 -p m2    # Manager同士は互いに指定する
[m2]$ kumo-manager -v -l m2 -p m1    # Manager同士は互いに指定する
[s1]$ kumo-server  -v -m m1 -p m2 -l s1 -s database.tch    # -mと-pでManagerを指定する
[s2]$ kumo-server  -v -m m1 -p m2 -l s2 -s database.tch
[s3]$ kumo-server  -v -m m1 -p m2 -l s3 -s database.tch
[s4]$ kumo-server  -v -m m1 -p m2 -l s4 -s database.tch
[c1]$ kumo-gateway -v -m m1 -p m2 -t 11211    # -mと-pでManagerを指定する
[c2]$ kumo-gateway -v -m m1 -p m2 -t 11211    # 11211/tcpでmemcachedテキストプロトコル
||<

すべてのプロセスを '''localhost''' で動かすには以下のようにします：
>|sh|
[localhost]$ kumo-manager -v -l localhost    # Serverはポートを変更して起動する
[localhost]$ kumo-server  -v -m localhost -l localhost:19801 -L 19901 -s database1.tch
[localhost]$ kumo-server  -v -m localhost -l localhost:19802 -L 19902 -s database2.tch
[localhost]$ kumo-gateway -v -m localhost -t 11211    # 11211/tcpでmemcachedテキストプロトコル
||<

**Serverノードの追加
動作中にServerノードを追加するには、新しくkumo-serverを起動し、''kumoctl''コマンドを使って登録します。

kumoctlコマンドにはManager（2台動作しているならどちらか片方）のアドレスを指定します。
>|sh|
[s5]$ kumo-server -v -m m1 -p m2 -l s5 -s database.tch    # 新しいServerを起動
[xx]$ kumoctl m1 status    # 新しいServerが認識されているか確認
[xx]$ kumoctl m1 attach    # 新しいServerをattach
||<

*障害からの復旧方法
kumofsでは、1つのkey-valueのペアは3台のServerノードにコピーされます。そのため2台までならServerノードがダウンしてもデータが欠損することなく動作し続けます。

またManagerノードは2台で冗長構成を取ることができます。片方がダウンしても正常に動作し続け、両方ダウンしてもその間にServerに障害が発生しなければ問題なく動作し続けます。

**Serverノードの復旧
Serverノードが1台ダウンすると、一部のkey-valueペアの複製が1つ減った状態のまま動作し続けます。2台ダウンすると、1つか2つ減った状態のままになります。この状態から複製の数を3つに戻すには、Serverノードを復旧させたあとkumoctlコマンドを使って登録するか、Serverノードを復旧させずにダウンしたServerノードを完全に切り離します。

まずkumoctlコマンドを使ってどのServerノードに障害が発生しているかを確認します：
>|sh|
[xx]$ kumoctl m1 status    # Managerのアドレスを指定して状態を取得
hash space timestamp:
  Wed Dec 03 22:15:35 +0900 2008 clock 50
attached node:
  192.168.0.101:19800  (active)
  192.168.0.102:19800  (active)
  192.168.0.103:19800  (active)
  192.168.0.104:19800  (fault)
not attached node:
||<
''(fault)''と表示されているServerノードに障害が発生しています。ここでServerノードを再起動すると、以下のようになります：

>|sh|
[xx]$ kumoctl m1 status
hash space timestamp:
  Wed Dec 03 22:15:45 +0900 2008 clock 58
attached node:
  192.168.0.101:19800  (active)
  192.168.0.102:19800  (active)
  192.168.0.103:19800  (active)
  192.168.0.104:19800  (fault)
not attached node:
  192.168.0.104:19800
||<
''not attached node''のところに表示されているServerノードは、Managerノードから認識されているが、まだ登録されていないServerノードの一覧です。

ここで''attach''コマンドを発行すると、Serverノードが実際に登録され、データの複製が3つになるようにコピーされます：
>|sh|
[xx]$ kumoctl m1 attach    # attach
[xx]$ kumoctl m1 status
hash space timestamp:
  Wed Dec 03 22:15:55 +0900 2008 clock 62
attached node:
  192.168.0.101:19800  (active)
  192.168.0.102:19800  (active)
  192.168.0.103:19800  (active)
  192.168.0.104:19800  (active)
not attached node:
||<

attachコマンドを発行して新たにServerノードを登録するとき、データの複製をコピーするための比較的大規模なネットワークトラフィックが発生することに注意してください。

kumoctlコマンドで、attachコマンドの代わりに''detach''コマンドを発行すると、fault状態のServerノードが切り離されます。このときも複製の数が3つになるようにデータのコピーが行われます。
>|sh|
[xx]$ kumoctl m1 detach    # detach
[xx]$ kumoctl m1 status
hash space timestamp:
  Wed Dec 03 22:15:55 +0900 2008 clock 62
attached node:
  192.168.0.101:19800  (active)
  192.168.0.102:19800  (active)
  192.168.0.103:19800  (active)
not attached node:
  192.168.0.104:19800
||<

FIXME Serverノードを復旧するときにのデータベースファイルの扱い deleteしたデータが復活する件

**Managerノードの復旧
2台で冗長構成を取っているときに片方のManagerノードがダウンした場合は、Managerノードを再起動してください。ManagerノードのIPアドレスは障害発生前と同じにしておく必要があります。

両方のManagerノードがダウンした場合や、Managerノードを1台で運用していた場合は、Managerノードを再起動し、kumoctlコマンドでattachコマンドを発行してください。

*チューニング
**Tokyo Cabinetのチューニング
Tokyo Cabinetのハッシュデータベースのチューニングによって性能は大きく変わります。kumo-serverを起動する前に、tchmgrコマンドを使ってデータベースファイルをあらかじめ作成しておいてください。
tchmgrコマンドのパラメータについては、Tokyo Cabinetのドキュメントを参照してください。
Tokyo Cabinetのパラメータのうち、拡張メモリマップのサイズ（xmsiz）とキャッシュ機構（rcnum）はkumo-serverのコマンドライン引数で指定します。kumo-serverの''-s''オプションで、データベースファイル名の後ろに''#xmsiz=XXX''と指定すると拡張メモリマップのサイズを指定できます。''#rcnum=XXX''と指定するとキャッシュ機構を有効化できます。
>|sh|
[s2]$ kumo-server  -v -m m1 -l s2 -s database.tch#xmsiz=600m#rcnum=4k
||<

**スレッド数
CPUのハードウェアスレッドの数が多い場合は、kumo-serverとkumo-gatewayのワーカースレッドの数（-TR引数）を増やすと性能が向上します。ハードウェアスレッド数+2 くらいが目安です。デフォルトは4です。
保存する1つ1つのkey-valueペアのサイズが大きい場合は、kumo-serverとkumo-gatewayの送信用スレッドの数（-TW引数）を増やすと性能が向上する可能性があります。デフォルトは2です。

// FIXME **タイムアウト時間の調節

**非同期レプリケーション
kumofsではデータをsetしたりdeleteしたりするときレプリケーションを行いますが、デフォルトではレプリケーションが完了するまで待ってから（すべてのサーバーから応答が帰ってきてから）アプリケーションに応答が返されます。これを1台のServerに書き込みが完了した時点で応答を返すようにすると（非同期レプリケーション）、更新系の応答時間が大幅に短縮されます。
ただし非同期レプリケーションを有効にすると、成功応答が帰ってきたとしても、必ずしもレプリケーションが成功していることが保証されず、そのため複数のServerノード間でデータの一貫性が保たれているとが保証されなくなります。
Set操作のレプリケーションを非同期にするには、kumo-gatewayのコマンドライン引数に''-As''を、Delete操作のレプリケーションを非同期にするには''-Ad''を追加してください。

*リファレンス
**保証範囲の制限
kumofsはスケーラビリティとアベイラビリティを持つ代わりに、複数のServerノード間に保存されるデータの一貫性に制限があります。アプリケーションで以下に挙げる保証範囲に含まれない動作を前提としないように注意してください。

***Set
Set操作はkey-valueペアを保存します。
Setが成功応答を返した場合は一貫性は保証されます。つまりすべての担当Serverノードに同じkey-valueペアが書き込まれ、どのノードからでも同じ値を直後にGetすることができ、古い値が読み出されることはありません。
Setが失敗応答を返した場合は一貫性は保証されません。つまりServerノードによって古い値を持っていたり新しい値を持っていたりします。
Set操作を実行中に同じkeyをGetした場合は、新旧どちらの値が読み出されるかは不定ですが、どちらかの値が読み出されます。混ざった値が読み出されることはありません。

***Delete
Delete操作はkey-valueペアを削除します。
Delete操作は一貫性を保証しませんが、できる限り一貫性が保たれるように努力します。
Delete操作は実際には「deleteフラグ」をSetする操作です。しかしdeleteフラグは時間が経過すると回収され、本当に削除されます。deleteフラグが回収されると一貫性は保証されません。deleteフラグは以下の条件で回収されます：
:Deleteしてから一定の時間が経過した:この時間はkumo-serverの''-gX''オプションで指定できます
:Deleteフラグを記憶するメモリ使用量の上限に達し、かつDeleteしてから一定の時間が経過した:この時間はkumo-serverの''-gN''オプションで指定できます。メモリ使用量の上限は''-gS''オプションで指定できます

deleteフラグを記憶するメモリ使用量の上限に達したが、Deleteしてから一定の時間が経過していない場合は、deleteフラグはデータベースファイルの中に放置されます。放置されたdeleteフラグは、次に再配置操作が行われたときに回収されます。

**ログ
kumo-manager, kumo-server, kumo-gatewayは、それぞれ2種類のログを出力します：
:テキストログ:行区切りのテキストフォーマットのログ。標準出力に出力される
:バイナリログ:MessagePackでシリアライズされたバイナリ形式のログ

テキストログは常に出力されます。''-v''オプションを付けると冗長なログも出力されるようになります。テキストログはファイルに書き出すこともできるが、ログローテーションはサポートしていません。デフォルトでは優先度によってログに色が付きますが、''-d <path.pid>''オプションを指定してデーモンとして起動するか、''-o "-"''オプションを指定すると、ログに色が付かなくなります。

バイナリログは''-g <path.mpac>''オプションを付けたときだけ出力されます。バイナリログはSIGHUPシグナルを受け取るとログファイルを開き直すため、logrotateなどを使ってログローテーションができます。
// FIXME logrotateの設定例

**共通のコマンドライン引数
:-o <path.log>:ログを標準出力ではなく指定されたファイルに出力する。''-''を指定すると標準出力に出力する。省略するとログに色を付けて標準出力に出力する
:-v:WARNよりレベルの低いメッセージを出力する
:-g <path.mpac>:バイナリログを指定されたファイルに出力する
:-d <path.pid>:デーモンになる。指定されたファイルにpidを書き出す
:-Ci <sec>:タイマークロックの間隔を指定する。単位は秒で、小数を指定できる
:-Ys <sec>:connect(2)のタイムアウト時間を指定する。単位は秒で、小数を指定できる
:-Yn <num>:connect(2)のリトライ回数を指定する
:-TR <num>:ワーカースレッドの数を指定する
:-TW <num>:送信用スレッドの数を指定する

**kumo-manager
:-l <address>:待ち受けるアドレス。''他のノードから見て''接続できるホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-c <port>:kumoctlからのコマンドを受け付けるポート番号を指定する
:-a:Serverが追加・離脱されたときに、マニュアル操作を待たずにレプリケーションの再配置を自動的に行うようにする。実行中でもkumoctlコマンドを使って変更できる
:-Rs:自動的な再配置が有効なときに、サーバーの追加・離脱を検出してからレプリケーションの再配置を開始するまでの待ち時間を指定する。単位は秒

**kumo-server
:-l <address>:待ち受けるアドレス。''他のノードから見て''接続できるホスト名とポート番号を指定する
:-L <port>:kumo-serverが待ち受けるもう一つのポートのポート番号を指定する
:-m <address>:kumo-managerのホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-s <path.tch[#xmsiz=SIZE][#rcnum=SIZE]>:データを保存するデータベースファイルのパスを指定する
:-f <dir>:レプリケーションの再配置に使う一時ファイルを保存するディレクトリを指定する。データベースファイルのサイズに応じて十分な空き容量が必要
:-gS <seconds>:deleteしたエントリのクロックを保持しておくメモリ使用量の上限をKB単位で指定する
:-gN <seconds>:deleteしたエントリのクロックを保持しておく最小時間を指定する。メモリ使用量が上限に達していると、最大時間に満たなくても最小時間を過ぎていれば削除される。
:-gX <seconds>:deleteしたエントリのクロックを保持しておく最大時間を指定する

**kumo-gateway
:-m <address>:kumo-managerのホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-t <port>:memcachedテキストプロトコルを待ち受けるポート番号を指定する
:-b <port>:memcachedバイナリプロトコルを待ち受けるポート番号を指定する（EXPERIMENTAL）
:-G <number>:Get操作の最大リトライ回数を指定する
:-S <number>:Set操作の最大リトライ回数を指定する
:-D <number>:Delete操作の最大リトライ回数を指定する
:-As:Set操作でレプリケーションするとき、レプリケーション完了の応答を待たずに成功を返すようにする
:-Ad:Delete操作でレプリケーションするとき、レプリケーション完了の応答を待たずに成功を返すようにする

**kumoctl
kumoctlコマンドはManagerノードに様々なコマンドを発行するための管理コマンドです。
第一引数にManagerノードのアドレスを指定し、第二引数にサブコマンドを指定します。
>|sh|
Usage: kumoctl address[:port=19799] command [options]
command:
   status                     get status
   attach                     attach all new servers and start replace
   attach-noreplace           attach all new servers
   detach                     detach all fault servers and start replace
   detach-noreplace           detach all fault servers
   replace                    start replace without attach/detach
   backup  [suffix=????????]  create backup with specified suffix
   enable-auto-replace        enable auto replace
   disable-auto-replace       disable auto replace
||<

***ハッシュ空間の取得
''status''サブコマンドは、どのServerにデータを保存するかを決定するハッシュ空間を取得します。以下のように表示されます：
>|sh|
hash space timestamp:
  Wed Dec 03 22:15:45 +0900 2008 clock 58
attached node:
  192.168.0.101:19800  (active)
  192.168.0.102:19800  (active)
  192.168.0.103:19800  (active)
  192.168.0.104:19800  (fault)
not attached node:
  192.168.0.104:19800
||<
'''hash space timestamp'''はハッシュ空間を更新した時刻を示しています。
'''attached node'''は登録されている（ルーティング対象の）Serverノードの一覧を示しています。'''(active)'''はそのノードが利用可能なことを、'''(fault)'''はそのノードがダウンしていることを示します。
'''not attached node'''は認識しているが登録されていないServerノードの一覧を示しています。

***Serverノードの追加と切り離し
''attach''サブコマンドは、認識しているが登録されていないServerノードを実際に登録します。''detach''サブコマンドは、fault状態のServerノードを切り離します。
''attach-noreplace''サブコマンドは''attach''と同じですが、Serverノードを登録した後にkey-valueペアの複製の再配置を行いません。''detach-noreplace''サブコマンドは''detach''と同じですが、Serverノードを切り離した後に複製の再配置を行いません。
''replace''サブコマンドは複製の再配置だけを行います。
attach-noreplaceサブコマンドとdetach-noreplaceサブコマンドはattachとdetachを同時に行いときのみ使用し、すぐにreplaceサブコマンドを使って再配置を行ってください。再配置を行わないまま長い間放置してはいけません。

***バックアップの作成
''backup''サブコマンドは、データベースファイルのバックアップを作成します。バックアップは認識しているすべてのServerノード上で作成されます。
バックアップファイルのファイル名は、元のデータベースファイル名に第三引数で指定したsuffixを付けたファイル名になります。suffixを省略するとその日の日付(YYMMDD)が使われます。
作成されたバックアップファイルは、''kumomergedb''コマンドを使って1つのファイルにまとめることができます。

**kumomergedb
kumomergedbコマンドを使うと複数のデータベースファイルを1つにまとめることができます。第一引数に出力先のファイル名を指定し、第二引数以降にまとめたいデータベースファイルを指定します。
>|sh|
$ kumomergedb backup.tch-20090101 \
              server1.tch-20090101 server2.tch-20090101 server3.tch-20090101
||<

**kumolog
kumologコマンドはバイナリ形式のログを人間にとって読みやすいテキストに変換して表示します。
>||
kumolog [options] <logfile.mpac>
||<

:-f, --follow     :  ''tail -f''と同じ効果
:-t, --tail       :  最後のN個のログだけ表示する（デフォルト: N=10）
:-h, --head       :  最初のN個のログだけ表示する（デフォルト: N=10）
:-n, --lines=[-]N :  Nを指定する

**kumostat
kumostatコマンドを使うとServerノードの状態を取得することができます。
第一引数にServerノードのホスト名とポート番号を指定し、第二引数にコマンドを指定します：
>||
Usage: kumostat address[:port=19800] command [options]
command:
   pid                        get pid of server process
   uptime                     get uptime
   time                       get UNIX time
   version                    get version
   cmd_get                    get number of get requests
   cmd_set                    get number of set requests
   cmd_delete                 get number of delete requests
   items                      get number of stored items
||<

:pid:kumo-serverプロセスのpid
:uptime:kumo-serverプロセスの起動時間（単位は秒）
:time:kumo-serverプロセスが走っているホストのUNIXタイム
:version:kumo-serverのバージョン
:cmd_get:GatewayノードからのGet操作を処理した回数
:cmd_set:GatewayノードからのSet操作を処理した回数
:cmd_delete:GatewayノードからのDelete操作を処理した回数
:items:データベースに入っているエントリの数

**kumotop
kumotopコマンドを使うとServerノードの状態を定期的に更新しながら表示することができます。
引数に監視したいServerノードのアドレスを指定します。Serverノードのアドレスは複数指定できます：
>||
Usage: kumotop address[:port=19800] ...
||<


*?kumofs internals
*分散アルゴリズム
kumofsはどのServerノードにデータを保存するかを決定するために、Consistent Hashingを利用しています。ハッシュ関数はSHA-1で、下位の64ビットのみを使います。1台の物理ノードは128台の仮想ノードを持ちます。

データを取得するときは、Gatewayがkeyにハッシュ関数を掛けてハッシュ空間から担当Serverノードを計算し、担当Serverノードからデータを取得します。取得に失敗したときは、ハッシュ空間上でその次に当たる物理Serverノードから取得します。それでも失敗したらその次の次のServerノードから取得します。それでも失敗したら担当Serverに戻ってリトライします。

ハッシュ関数はGatewayで計算することでServerノードの負荷は下げています。

データを変更するときは、Gatewayがkeyにハッシュ関数を掛けてハッシュ空間から担当Serverノードを計算し、担当Serverノードにデータを送信します。取得する場合とは異なり、次のServerにフォールバックすることはありません。

担当Serverノードは変更操作を受け取ると、データをハッシュ空間上で次の物理Serverノードと、次の次の物理Serverノードにもkey-valueペアをコピーします。

ServerノードはGatewayからリクエストを受け取ったとき、本当に自分が担当ノードであるかどうかを自分が持っているハッシュ空間を使って確認します。間違っていた場合はリクエストを拒否します。このように必ず特定の担当Serverノードだけがデータを変更でき、他のServerノードが同じタイミングで同じkey-valueを変更することがないようになっています。

Serverノードを選ぶときfaultフラグがセットされているノードはスキップします。一部のServerノードがダウンしている状態でも正常なアクセスを続けられるようになっています。

**Serverノードの追加

*死活監視と障害検出
**障害の検出
メッセージを送ろうとしたところ、接続済みのすべてのTCPコネクションでエラーが発生し、再接続を試みても失敗して再接続のリトライ回数が上限に達したら、そのノードはダウンしたと判断します。
TCPコネクションが切断されただけではダウンしたとは判断せず、メッセージの送信に失敗しても制限回数以内に再接続することができたら再送されます。
ServerノードとManagerノードは常にkeep-aliveメッセージをやりとりしており、いつもメッセージを送ろうとしている状態になっています。Serverノードがダウンしたらできるだけ早く検出してfaultフラグをセットし、正常なアクセスを継続させます。

**新しいノードの検出

*再配置アルゴリズム


// vim syntax=wikiforme
