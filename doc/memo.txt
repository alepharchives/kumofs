?title Kumofs memo -- Kumo Fast Storage rev.1012
?author FURUHASHI Sadayuki

?css user.css

*概要
kumofsはkey-value型のデータを保存する分散ストレージ。key=>valueを保存する''set''、keyを取得する''get''、keyを削除する''delete''の3つの操作をサポートする。
データを保持する''Server''、Server群を管理する''Manager''、アプリケーションからのリクエストをServerに中継する''Gateway''の3種類のノードでシステムを構成する。
データは複数のServerに分散して保存されるため、Serverを追加するほど性能が向上する。
データは3台のServerにコピーされて保存される。2台までならServerがダウンしても動作し続ける。
Server群はManagerによって死活監視されている。Serverがダウンしたら、そのServerは直ちにシステムから切り離される。ただし1台か2台のManagerが起動していないとServerの切り離しが行われないので、Managerが1台も起動していない状態でServerがダウンするとシステムが停止してしまう。
Serverを追加したり切り離したりした後、その状態をシステムに反映するには、レプリケーションされたデータの再配置を行う必要がある。これは自動では行われず、''kumoctlコマンド''を使って手動で行う。


**Consistent Hashing
Consistent Hashingを使ってデータを分散して保存する((ハッシュ関数はSHA-1で、下位64ビットのみ使う。仮想ノードは128台))。
Serverがダウンしたときは、そのServerの仮想ノードに''faultフラグ''がセットされる。set/get/deleteはfaultフラグがセットされたServerをスキップして行われる。つまり、通常動作時はレプリケーションは3つ作成されるが、1台がfault状態ならコピーは2つ、2台がfault状態ならコピーは1つしか作成されないkeyが存在することになる。fault状態のServerが3台以上になると、get/set/deleteが失敗し続けるkeyが存在することになる。
Serverがダウンしてもfaultフラグがセットされるだけで、レプリケーションの再配置は行われない。faultフラグがセットされたServerが存在する状態で、kumoctlコマンドを使って''detach''コマンドをManagerに送信すると、faultフラグがセットされたServerがハッシュ空間から取り除かれる。同時にレプリケーションの再配置が行われ、すべてのkeyに対してレプリケーションが3つ作成されるようにデータがコピーされる。
Serverが追加されてもすぐにはハッシュ空間には追加されず、レプリケーションの再配置は行われない。新たなServerが起動している状態で、kumoctlコマンドを使って''attach''コマンドをManagerに送信すると、新しいServerがハッシュ空間に追加される。同時にレプリケーションの再配置が行われ、すべてのkeyに対してレプリケーションが3つだけ存在するようにデータが移動される。

# TODO: auto-replace

**set/get/deleteの保証範囲
***set(key, value)
key=>valueを保存する。保存できれば成功を返す。保存できなければエラーを返す。
既にkeyが保存されていたとき、setが成功した場合はkeyの値は確実に上書きされている。
setが失敗したとき、keyの値は不定になっている。これは失敗したときにロールバックを行わないため。ロールバックを一貫性を損なうことなく行うための高級なアルゴリズムは実装されていない/使うと性能が低下してしまう。
Serverはレプリケーション先の2台〜0台のすべてのServerにデータが受信されたことを確認してからGatewayにレスポンスを返す。どれか1台でもコピー処理が失敗したらエラーを返す。つまり、アプリケーションに成功が返されたときはfault状態でないすべてのServerにレプリケーションがコピーされており、それ以降に古いデータが読み出されることはない。ただしディスクに書き込まれているとは限らない。

***get(key)
keyをsetするリクエストが成功していた場合は、そのkeyに対応するvalueを返す。setが失敗していた場合は、nullか、setに失敗したvalueが返る。それ以外であればnullを返す。
keyをsetするリクエストが成功してvalueが保存されていたとしても、レプリケーションされたすべてのServerの負荷が非常に高いために応答できない場合は、getがタイムアウトする可能性がある。
keyが保存されていなかった場合はエラーにならないが、タイムアウトした場合はエラーになる。

***delete(key)
keyを削除する。
deleteを行ってもkeyが削除されないことがある。
Serverはkeyがいつ削除されたかを記憶しているが、時間が経過すると記憶を破棄する。一斉に大量にkeyが削除されるとメモリ使用量の上限に達し、より速く記憶を破棄するようになる。このとき遅延していたレプリケーションや、遅延していた再配置が介入すると、deleteされるはずのkeyが古いkeyによって上書きされる。
記憶を破棄するまでの時間とメモリの使用量はServerの引数で指定できる。


**動作環境と制限
***サーバーの時刻設定
ManagerとServerを動作させるホストの時刻設定は、TIME_ERROR_MARGIN秒（コンパイル時に決定。デフォルトでは5秒）以上ずれていると正常に動作しない。またUTCとlocaltimeはどちらかに揃える必要がある。

# TODO


@footnote


*インストールと実行
**依存関係
***動作環境
-linux >= 2.6.18
-glibc >= XXX

***コンパイル時に必要なもの
-g++ >= 4.1
-ragel >= 6.3

***コンパイル時と実行時に必要なもの
-ruby >= 1.8
-libcrypto(openssl)
-zlib >= XXX
-Tokyo Cabinet >= 1.4.10
-MessagePack for C++ >= 0.3.1
-MessagePack for Ruby >= 0.3.1

**コンパイル
シンプルに./configure && make && make installでインストールできる。
>|sh|
$ ./configure && make && make install
||<

以下の4つのコマンドがインストールされる：
:kumo-manager:Managerノード。Serverノードの管理をする。
:kumo-server:Serverノード。実際にデータを保存する。
:kumo-gateway:Gatewayノード。memcachedプロトコルのサーバーで、アプリケーションからの要求をServerノードに中継する。
:kumoctl:Managerノードを制御するための管理コマンド
:kumostat:Serverノードの状態を取得する
:kumotop:Serverノードの状態を定期的に更新しながら表示する
:kumolog:バイナリフォーマットのログをテキストフォーマットに変換する
:kumomergedb:コールドバックアップファイルをマージする

***configureフラグ
:--with-msgpack=DIR:MessagePackがインストールされているディレクトリを指定する
:--with-tokyocabinet=DIR:Tokyo Cabinetがインストールされているディレクトリを指定する
:--enable-trace:画面を埋め尽くすほど冗長なデバッグ用のメッセージを出力するようにする
:--with-tcmalloc[=DIR]:tcmallocとリンクする

**実行例
***Manager 2台, Server 4台を使った冗長構成
''s1''〜''s4''の4台でクラスタを構成し、''c1''と''c2''で動作するアプリケーションから利用する例。
s1〜s4でServerを起動し、''s1''と''s2''では同時にManagerも起動する。''c1''と''c2''ではGatewayを起動する。
>|sh|
[s1]$ kumo-manager -v -l s1 -p s2    # Manager同士は互いに指定する
[s2]$ kumo-manager -v -l s2 -p s1    # Manager同士は互いに指定する
[s1]$ kumo-server  -v -m s1 -p s2 -l s1 -s database.tch    # -mと-pでManagerを指定する
[s2]$ kumo-server  -v -m s1 -p s2 -l s2 -s database.tch    # -lは常に自ホストのアドレス
[s3]$ kumo-server  -v -m s1 -p s2 -l s3 -s database.tch    # -sはデータベース名
[s4]$ kumo-server  -v -m s1 -p s2 -l s4 -s database.tch    # -vは冗長なメッセージを出力
[c1]$ kumo-gateway -v -m s1 -p s2 -t 11211    # 11211/tcpでmemcachedテキストプロトコル
[c2]$ kumo-gateway -v -m s1 -p s2 -t 11211    # を待ち受ける
||<

***localhostでクラスタを構成する
localhostでManagerノード1台、Server 2台を使ってクラスタを構成する例。
>|sh|
[localhost]$ kumo-manager -v -l localhost   # Managerを1台で運用するときは-pを省略
                           # kumo-serverはポートを変えて起動する
[localhost]$ kumo-server  -v -m localhost -l localhost:19801 -L 19901 -s database1.tch
[localhost]$ kumo-server  -v -m localhost -l localhost:19802 -L 19902 -s database2.tch
[localhost]$ kumo-gateway -v -m localhost -t 11211
||<


**主な引数
**共通
:-o <path.log>:ログを標準出力ではなく指定されたファイルに出力する
:-g <path.mpac>:バイナリログを指定されたファイルに出力する
:-d <path.pid>:デーモンになる。指定されたファイルにpidを書き出す
:-v:WARNよりレベルの低いメッセージを出力する
:-Ci <sec>:タイマークロックの間隔を指定する。単位は秒で小数を指定できる
:-Ys <sec>:connect(2)のタイムアウト時間を指定する。単位は秒で小数を指定できる
:-Yn <num>:connect(2)のリトライ回数を指定する
:-TR <num>:送信用スレッドの数を指定する
:-TW <num>:受信用スレッドの数を指定する

***kumo-manager
:-l <address>:待ち受けるアドレス。''他のノードから見て''接続できるホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-c <port>:kumoctlからのコマンドを受け付けるポート番号を指定する
:-a:Serverが追加・離脱されたときに、マニュアル操作を待たずにレプリケーションの再配置を自動的に行うようにする。実行中でもkumoctlコマンドを使って変更できる
:-Rs:自動的な再配置が有効なときに、サーバーの追加・離脱を検出してからレプリケーションの再配置を開始するまでの待ち時間を指定する。単位は秒

***kumo-server
:-l <address>:待ち受けるアドレス。''他のノードから見て''接続できるホスト名とポート番号を指定する
:-L <port>:kumo-serverが待ち受けるもう一つのポートのポート番号を指定する
:-m <address>:kumo-managerのホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-s <path.tch[#xmsiz=SIZE][#rcnum=SIZE]>:データを保存するデータベースファイルのパスを指定する
:-f <dir>:レプリケーションの再配置に使う一時ファイルを保存するディレクトリを指定する。データベースファイルのサイズに応じて十分な空き容量が必要
:-gS <seconds>:deleteしたエントリのクロックを保持しておくメモリ使用量の上限をKB単位で指定する
:-gN <seconds>:deleteしたエントリのクロックを保持しておく最小時間を指定する。メモリ使用量が上限に達していると、最大時間に満たなくても最小時間を過ぎていれば削除される。
:-gX <seconds>:deleteしたエントリのクロックを保持しておく最大時間を指定する

***kumo-gateway
:-m <address>:kumo-managerのホスト名とポート番号を指定する
:-p <address>:もし存在するなら、もう一台のkumo-managerのホスト名とポート番号を指定する
:-t <port>:memcachedテキストプロトコルを待ち受けるポート番号を指定する
:-b <port>:memcachedバイナリプロトコルを待ち受けるポート番号を指定する（EXPERIMENTAL）
:-G <number>:getの最大リトライ回数を指定する
:-S <number>:setの最大リトライ回数を指定する
:-D <number>:deleteの最大リトライ回数を指定する
:-As:set操作でレプリケーションするとき、レプリケーション完了の応答を待たずに成功を返すようにする
:-Ad:delete操作でレプリケーションするとき、レプリケーション完了の応答を待たずに成功を返すようにする

@footnote


*ゲートウェイ
**memcached text protocol gateway
サポートしているプロトコルは以下の通り：

-get (複数key対応。ただしflagsは常に0になる)
-set (ただしflagsとexptimeは0でないとエラーになる)
-delete (ただしexptimeは0でないとエラーになる)


**memcached binary protocol gateway
サポートしているプロトコルは以下の通り：

-get    (ただしcas uniqueは0でないとエラーになる)
-getq   (ただしcas uniqueは0でないとエラーになる)
-getk   (ただしcas uniqueは0でないとエラーになる)
-getkq  (ただしcas uniqueは0でないとエラーになる)
-set    (ただしcas uniqueとflagsとexptimeは0でないとエラーになる)
-delete (ただしcas uniqueとexptimeは0でないとエラーになる)
-noop

リクエストのパイプライン化にも対応している。


*管理コマンド
**kumoctl
kumoctlコマンドを使うとManagerの状態を取得したり、コマンドを送ったりできる。
Rubyで書かれたスクリプト。実行するにはgemでmsgpackパッケージをインストールする。
第1引数にManagerのホスト名とポート番号を指定し、第2引数にコマンドを指定する。
>|sh|
$ kumoctl --help
Usage: kumoctl address[:port=19799] command [options]
command:
   status                     get status
   attach                     attach all new servers and start replace
   attach-noreplace           attach all new servers
   detach                     detach all fault servers and start replace
   detach-noreplace           detach all fault servers
   replace                    start replace without attach/detach
   backup  [suffix=????????]  create backup with specified suffix
   enable-auto-replace        enable auto replace
   disable-auto-replace       disable auto replace
||<

***status
Managerが持っているハッシュ空間を取得して表示する。
>||
$ kumoctl localhost status
hash space timestamp:
  Wed Dec 03 22:15:45 +0900 2008 clock 58
attached node:
  127.0.0.1:8000  (active)
  127.0.0.1:8001  (fault)
not attached node:
  127.0.0.1:8002
||<
^title statusの実行例
''attached node''はハッシュ空間に入っているServerの一覧を示している。''(active)''は正常動作中のServerで、''(fault)''はfaultフラグが立っているServerを示している。
''not attached node''はハッシュ空間に入っていないか、入っているが(fault)状態でまだ再attachされていないServerの一覧を示している。

レプリケーションの再配置を行ったとき、Managerが２台起動していれば２つのManager間で新しいハッシュ空間が同期される。ただし新しいハッシュ空間が空の時は同期されない。
この理由は、障害が発生していたManagerを復旧したときに空のハッシュ空間が同期されてしまう可能性があるため。起動した直後はクロック（後述）が調整されていないために、ハッシュ空間の新旧の比較が正常に機能しない。このため空のハッシュ空間を受け取ったときは無視するようになっている。 # FIXME この動作は正しい？もっと良い回避方法は無いか？


***attach
statusで''not attached node''に表示されているServerをハッシュ空間に組み入れ、レプリケーションの再配置を開始する。

***attach-noreplace
attachと同じだがレプリケーションの再配置を開始しない。ただし再配置をしないまま長い間放置してはいけない。
再配置を行わないと、エラーが積もってGatewayから最新のハッシュ空間を要求されたとき（後述）、Gatewayが持っているハッシュ空間とServerが持っているハッシュ空間が食い違ってしまう。食い違うとsetやdeleteがいつまで経っても成功しなくなってしまう。

***detach
statusで''attached node''に表示されていて(fault)状態のServerをハッシュ空間から取り除き、レプリケーションの再配置を開始する。

***detach-noreplace
detachと同じだがレプリケーションの再配置を開始しない。再配置をしないまま長い間放置してはいけない。

***replace
レプリケーションの再配置を開始する。

***backup
コールドバックアップを作成する。バックアップはServerで作成され、元のデータベース名にsuffixを付けた名前のファイルにデータベースがコピーされる。手元にバックアップを持ってくるには、rsyncやscpなどを使ってServerからダウンロードする。
suffixは省略するとその日の日付（YYMMDD）が使われる。
作成されたバックアップファイルは、kumomergedbコマンドを使って１つのファイルに結合することができる。
>|sh|
$ kumomergedb backup.tch-20090101 \
              server1.tch-20090101 server2.tch-20090101 server3.tch-20090101
||<
^title kumomergedbコマンドの実行例


**kumostat
kumostatコマンドを使うとServerの状態を取得することができる。
Rubyで書かれたスクリプト。実行するにはgemでmsgpackパッケージをインストールする。
第1引数にServerのホスト名とポート番号を指定し、第2引数にコマンドを指定する。
>||
Usage: kumostat server-address[:port=19800] command
       kumostat -m manager-address[:port=19700] command
command:
   pid                        get pid of server process
   uptime                     get uptime
   time                       get UNIX time
   version                    get version
   cmd_get                    get number of get requests
   cmd_set                    get number of set requests
   cmd_delete                 get number of delete requests
   items                      get number of stored items
||<

:pid:kumo-serverプロセスのpidを取得する
:uptime:kumo-serverプロセスの起動時間を取得する。単位は秒
:time:kumo-serverプロセスが走っているホストのUNIXタイムを取得する
:version:バージョンを取得する
:cmd_get:GatewayからのGetリクエストを処理した回数を取得する
:cmd_set:GatewayからのSetリクエストを処理した回数を取得する
:cmd_delete:GatewayからのDeleteリクエストを処理した回数を取得する
:items:データベースに入っているエントリの数を取得する

''-m''オプションを指定すると、ManagerからServer一覧を取得してすべてのServerの状態を表示する。


**kumolog
kumologコマンドを使うとバイナリログをテキストに変換することができる。
>||
Usage: kumolog <logfile.mpac>
||<


**kumotop
kumotopコマンドを使うとServerの状態を定期的に更新しながら表示することができる。
引数に監視したいServerのアドレスを指定する。Serverのアドレスは複数指定できる。
>||
Usage: kumotop address[:port=19800] ...
||<


*ログ
kumo-manager, kumo-server, kumo-gatewayは、それぞれ2種類のログを出力する:
:テキストログ:行区切りのテキストフォーマットのログ。通常標準出力に出力される
:バイナリログ:MessagePackでシリアライズされたログ

テキストログは常に出力される。''-v''オプションを付けると冗長なログも出力されるようになる。テキストログはファイルに書き出すこともできるが、ログローテーションはサポートしていない。''-d <path.pid>''オプションを指定してデーモンとして起動するか、''-o -''オプションを指定すると、ログに色が付かなくなる。

バイナリログは''-g <path.mpac>''オプションを付けたときだけ出力される。''-v''オプションは影響しない。バイナリログはSIGHUPシグナルを受け取るとログファイルを開き直すため、logrotateなどを使ってログローテーションができる。

バイナリログは''kumolog''コマンドを使ってテキストに変換して読むことができる。
>|sh|
$ kumolog manager.mpac
||<
^title kumologコマンドの実行例


*チューニング
**データベースのチューニング
Tokyo Cabinetのチューニングによって性能が大きく変わる。kumo-serverを起動する前にあらかじめ''tchmgr''コマンドでデータベースファイルを作成しておく。
チューニングのパラメータはTokyo Cabinetのドキュメント参照。http://tokyocabinet.sourceforge.net/spex-ja.html
>|sh|
$ tchmgr create /path/to/database.tch 1048568  # バケット数を2097136個にして作成
$ kumo-server -m localhost -s /path/to/database.tch
||<

**キャッシュサイズのチューニング
Tokyo Cabinetの一部のパラメータは起動時に指定する。
 -sオプションにしているパスの後ろに''#xmsiz=XXX''と指定すると、拡張マップメモリのサイズを指定できる。''#rcnum=XXX''と指定すると、キャッシュ機構を有効化できる。両方指定するには''#xmsiz=XXX#rcnum=XXX''と指定する。XXXは整数か小数で、k,m,gなどの単位を使える。
拡張メモリマップとキャッシュ機構の効果についてはTokyo Cabinetのドキュメント参照。

**タイムアウト時間とkeepalive間隔の調整
# TODO


*死活監視と再配置
**障害の検出
ManagerとServerの接続では、あるノードにリクエストまたはレスポンスを送信しようとしたときに、そのノードとのコネクションが一本も存在せず、さらにconnect(2)が4回((--connect-retry-limitで指定))連続して失敗したら、そのノードはダウンしたと見なす。
ManagerとServerは2秒間隔((--keep-alive-interval引数で指定))でkeepaliveメッセージをやりとりしているので、いつも何らかのリクエストかレスポンスを送ろうとしている状態になっている。
connect(2)は次の条件で失敗する：
-接続相手から明示的に接続を拒否された（Connection Refused）
-接続相手からの応答がない時間が3ステップ((--connect-timeout-steps引数で指定))続いた。1ステップは0.5秒((--clock-interval引数で指定))


**接続の検出
ManagerとServerの接続では、あるノードから接続を受け付けた後、そのノードから初期ネゴシエーションメッセージを受け取り、かつそのメッセージのフォーマットが正しければ、そのノードが新たに起動したと見なす。


**ハッシュ空間の更新
Consistent Hashingのハッシュ空間を更新できるのはManagerだけで、最新のハッシュ空間は常にManagerが持っている。
通常動作時には1種類のハッシュ空間しか存在しないが、レプリケーションの再配置を行っている間は2種類のバージョンが存在する。最新のもの（Serverの追加/切り離しの更新が反映されている）は''whs''、1つ前のバージョン（Serverの追加/切り離しの更新が反映されていない）は''rhs''という名前が付いている。


Managerはkumoctlコマンドでレプリケーションの再配置を行うように指令されると、まずServerの追加/切り離しをwhsに反映する。もう1台のManagerが存在すればそのManagerに更新したwhsを送信する。
次に認識しているすべてのServerにwhsを送信し、レプリケーションのコピーを行うようにコマンドを送る。Serverは自分が持っているwhsとManagerから送られてきたwhsを比較し、必要なら他のServerにデータのコピーを行う（このときデータベースを上から下まで読み込む）。Serverはコピーが終わったらwhsをrhsにコピーする。
Serverはすべてのデータを確認し終えたら、Managerにコピーが終了した旨を通知する。ManagerはすべてのServerでコピーが終了した通知を受け取ったら、whsをrhsにコピーする。また、認識しているすべてのサーバーにレプリケーションの削除を行うようにコマンドを送る。Serverはwhsを参照して、自分が持っている必要がないデータがデータベースの中に入っていたら、それを削除する（このときもデータベースを上から下まで読み込む）。

Managerはレプリケーションのコピーを行っている最中にServerがダウンしたことを検知したら、すべてのServerからレプリケーションのコピーが終了した通知を受け取っても、レプリケーションの削除を行わない。

ServerはGatewayからget/set/deleteリクエストを受け取ったとき、そのkeyに対する割り当てノードが本当に自分であるか確認するために、getの場合はrhsを、set/deleteの場合はwhsを参照する。


**レプリケーションの再配置アルゴリズム
# TODO レプリケーションの再配置アルゴリズム
logic/srv_replace.cc:Server::replace_copy()


*レプリケーション
**set/deleteの伝播
Gatewayにsetリクエストを送信すると、keyにハッシュ関数を適用してハッシュ空間から検索し、一番最初にヒットしたServerに対してsetリクエストが送信される。
setリクエストを受け取ったServerは、keyのハッシュをハッシュ空間から検索し、自分が確かに最初にヒットするServerかどうか確かめる。そうでなければGatewayに「ハッシュ空間が古いぞ」とエラーを返す。
次にServerは、自分の次のServerと次の次のServerにデータをコピーする。このときコピー先のServerにfaultフラグが立っていたら、そのServerにはコピーしない。

Gatewayはset/deleteが何回失敗しても、次のServerにフォールバックすることはない。set先のServerが別のServerに切り替わるのは、Managerから新しいハッシュ空間を届いたときのみ。

以上の仕組みから、あるkeyをset/deleteするときは必ず単一のServerを経由することになる。このためほぼ同時にset/deleteされても必ず順序が付けられ、常に最新の結果がだけが残る。


**getのフォールバック
Gatewayはgetリクエストがタイムアウトしたり失敗したりすると、ハッシュ空間上の次のServerにリクエストする。それでもタイムアウトしたときは次の次のServerにリクエストする。リトライ回数の上限に達するまで、最初のServer→次のServer→次の次のServer→最初のServer→…とリトライが繰り返される。

getはManagerから新しいハッシュ空間が届くのを待つことなくフォールバックする。


**タイムアウト
GatewayでもServerでもManagerでも、リクエストを送ってから10ステップ（1ステップは0.5秒((--clock-interval引数で指定))）の間にレスポンスが返ってこないと、そのリクエストはタイムアウトしてエラーになる。
プログラムから見てTCPコネクションが確立しているか否かはタイムアウトには関係しない。コネクションが確立していなくても時間以内に再接続してレスポンスが返れば正常通り処理が続行され、コネクションが確立していても時間以内にレスポンスが返ってこなければタイムアウトする。

GatewayはServerに送ったリクエストがエラーになった回数が5回((--renew-threashold引数で指定))以上失敗すると、Managerから最新のハッシュ空間を取得する。


**リトライ
Gatewayはsetは最大20回((--set-retry引数で指定))まで、deleteは最大20回((--delete-retry引数で指定))まで、getは最大5×(レプリケーション数==3 + 1)回((係数は--get-retry引数で指定))までリトライする。制限回数までリトライしても失敗したらアプリケーションにエラーが返される。

@footnote


*クロック
データベースに保存されているすべてのvalueや、ハッシュ空間には、クロック（=タイムスタンプ）が付与されている。value同士やハッシュ空間同士でどちらが新しいかを比べるために利用している。
ref:[[Lamport Clockの解説>http://funini.com/kei/logos/clock.shtml]]

**クロックのフォーマット
クロックは64ビットの整数で、上位32ビットにはUNIXタイム（精度は秒）、下位32ビットにはLamport Clockが入っている。
UNIXタイムが上位に入っているので、Server/Manager同士の時刻が1秒以上ずれていると、Lamport Clockに関係なく間違った比較が行われてしまう。

**データベースのフォーマット
データベースにkeyを保存するとき、先頭の64ビットにkeyのハッシュを負荷して保存する。
データベースにvalueを保存するとき、先頭の64ビットにクロックを付加して保存する。またその次の64ビットも予約してあるが、使っていない。
>||
Database entry format
Big endian

key:
+--------+-----------------+
|   64   |       ...       |
+--------+-----------------+
hash
         key

value:
+--------+--+-----------------+
|   64   |16|       ...       |
+--------+--+-----------------+
clocktime
         meta
            data
||<

**レプリケーションでの利用
Serverから別のServerにデータをコピーするとき、後から来たsetリクエストのレプリケーションが、先に来たsetリクエストのレプリケーションを追い抜いて先行してしまうことが発生し得る。Serverはレプリケーションを受け取ったとき、既に保存されているvalueのクロックと新たに届いたvalueのクロックを比べ、新たに届いた方が新しかった場合のみデータベースを更新する。
レプリケーションの再配置を行うとき、ほとんどの場合はレプリケーションされたどのServerも同じデータを持っているが、setが失敗していた場合は異なるデータを持っている可能性がある。このときどのServerが持っているデータが最新なのか比べる必要があり、クロックを利用して比較する。

**Manager間の協調動作での利用
Managerが2台動作しているとき、どちらが持っているハッシュ空間が最新なのかを比べる必要がある。ハッシュ空間を更新するときに更新した時のクロックを付与しておき、比較するときにこのクロックを利用する。

@footnote

